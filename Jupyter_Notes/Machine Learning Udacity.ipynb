{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACINE LEARNING INTRO UDACITY\n",
    "\n",
    "---\n",
    "\n",
    "In machine learning we input **features** and we output **labels**. When we are training our classifier it is important that we train and test our classifier with different data if not this could couse an overfitting problem.\n",
    "\n",
    "### Bias-Variance dilemma\n",
    "\n",
    "#### BIAS:\n",
    "- Pays low attention to data.\n",
    "\n",
    "#### VARIANCE:\n",
    "- Very perceptive to data (reacts very poorly to data that hasn't seen, is more like memoryzing what is ghas seen).\n",
    "\n",
    "---\n",
    "# Supevised Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "### Naive Bayes \n",
    "\n",
    "Is an algorithm to obtain the decision surface line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB() # We create a classifier\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to calcultae the **accuracy** in a naive bayes classifier is by observing the number of points classified correctly divided by the number of points in the data set. In this case we can't actually do it as we don't see the naive bayes. there is a module in sklearn that can help us to do this \n",
    "``` python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_data, predicted_data)\n",
    "```\n",
    "\n",
    "\n",
    "#### Bayes Rule\n",
    "\n",
    "P( X | Y ) can be seen as the probability of X being Y correct:\n",
    "\n",
    "**Joint: **$P(X,Y)=P(X)*P(Y|X)$\n",
    "\n",
    "**Normalizer: **$P(Y)=P(X|Y)+P(¬X|Y)$\n",
    "\n",
    "$P(X|Y)={P(X,Y)}/{P(Y)} = {P(X)*P(Y|X)}/{P(Y)}$\n",
    "\n",
    "\n",
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Maximizes the distance to the nearest point of the different classes, this distance is called the **margin**. By doing this we maximize the robustness of our results. There are different SVM's **linear** or **non-linear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\") # There are 4 differnt types of kernels\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf.fit(X, y) \n",
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are treating with non-linear SVM's what we are doing is king of adding another feature to the SVM, before we had the **X** and **Y** axis that helped us difference different classes linearly. Now we would have also **Z**.  $\\ Z=X^2+Y^2$ so Z will be the distance from the center to the point.\n",
    "\n",
    "**SVM PARAMETERS**\n",
    "- **C**: Low C makes the decision surface smooth, this means it doesn't matter if there are wrong training classifications, high C aims to classify all the training data correctly.\n",
    "- **gamma**: Stablishs the influence of the different points when deciding the decision boundary of the SVM, low values mean that also the far point to the decision boundary are taken into account and a high value means that as closer they are to the decision boundary more influence they have.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Descision trees\n",
    "\n",
    "The decision trees allow us to make not linearbly separable data linearly separable. Segmenting the data into different decision. The problem of decision trees is that they tend to overfight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy \n",
    "\n",
    "The entropy helps us to calculate the impurenes of data.\n",
    "\n",
    "$Entropy= -\\sum Pi*\\log_2(Pi)$\n",
    "\n",
    "Pi will be the probability of case i of occuring and the summatory will go through all of the posibilities. For exampleif we are throwing a coin there will be 2 Pi's, heads and tails. It will give an entropy of 1.  \n",
    "\n",
    "#### Information Gain\n",
    "\n",
    "This is usefull to know the information gain of a parent with his childs. The value goes from 0 to 1. 1 means that the information gained between parent and child is maximum what means that there results are totally related.\n",
    "\n",
    "$Information\\ Gain = Entropy\\_Parent - Weighted\\_Average * Entropy\\_Children$ \n",
    "\n",
    "#### Decision tree parameters\n",
    "- **min_samples_split**: stablishs When does the tree stop spliting the data. \n",
    "\n",
    "---\n",
    "\n",
    "### K Nearest Neighbours\n",
    "\n",
    "Basically this algorithm divides the region depending on the elements that it has nearer, K indicates the number of elements that is will look for having it closer, increasing K can help us reduce overfitting. In the image bellow it shows a distribution with **k = 1** at the left and with **k = 3** at the right.\n",
    "![KNN Algoritm](KNNrepresentation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.66666667  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = [[0],[1],[2],[3]]\n",
    "Y = [0,0,1,1]\n",
    "clf = KNeighborsClassifier(n_neighbors=3) #Stablishs K\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[1.1]]))\n",
    "print(clf.predict_proba([[0.9]])) #Returns the probability of being 0 and of being 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN parameters\n",
    "- **n_neihbors**: Stablishs K.\n",
    "- **weights**: _'uniform'_ all the neighbours have the same weight (default) and _'distance'_ the neighbours that are closer have a higher weight.\n",
    "- **algorithm**: here it is specified the algorithm used to obtain the nearest trees the default is _'auto'_ that decides the most appropiate algorithm based on the values.\n",
    "- **leaf_size**: default = 30, it is passed to BallTree or KDTree that are the algorithms used to compute the nearest tree.\n",
    "\n",
    "---\n",
    "\n",
    "### Random forest\n",
    "\n",
    "The idea of random forest is to add randomness to the decision trees in order to avoid overfitting, reducing the variance of the simple decision trees. Random forest generates different decision trees with the use of random data after that to predict new data it makes a decision based on the result obtained by all the trees and the predominant opinion will be the selected one. [Link](https://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)\n",
    "\n",
    "#### Randomness\n",
    "- **At row level**: A percentage of the data is selected for every tree for example 10% so every random tree generated will use 10% of the data randomly selected. The dataset used for each random forrest is called a bootstrap data set.\n",
    "- **At column level**: It selects X random features of the decision trees, then the root of that decision tree will be one of those X features, the one that better separates the samples of the decision tree. It will do this until it has builed the hole decision trees always selecting between X random features instead of selecting between all of them. Brieman suggests tree posible values for X, being m the number of remaining features: $1/2*\\sqrt m ,\\ \\sqrt m\\ and\\ 2*\\sqrt m  $. Being X allways lower than the number of remaining features (m) except when there is just one feature (m == 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)\n",
    "print(clf.predict([[0.5,0.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest parameters\n",
    "- **n_estimators**: Number of trees in the forest (default=10).\n",
    "- **criterion**: Is the criteria used to determine which is the best split_'gini'_ (default) or _'entropy'_.\n",
    "- **max_features**: Is the number of features to consider when doing the next split (default=_'auto', max_features = sqrt(num_features)).\n",
    "\n",
    "---\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "Aims to convert a set of weak classifiers into a strong one. It uses weighted classifiers and improves them. What it does is basically it classify the data given and analyzes the error after analyzing them the weight of the correct values is decreased and the weight of th incorrect values is increased so that the algorithm priortizes the ones that weok worst, to try to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = AdaBoostClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "print(clf.predict([[1,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost parameters\n",
    "- **base_estimator**: It stablishs the classifier that will be used as weak classifier to be boosted (default=_'DecisionTreeClassifier'_)\n",
    "- **n_estimators**: The maximum number of estimators created, if perfect fit it stops (default = 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Continuous Supervised Learning\n",
    "\n",
    "The continuous concept, makes reference to the output of the algoithms that previously gave us a discrete output (can be individually separated and distincted, unordered), while now we will work with continuous output.\n",
    "\n",
    "### Linear Regression\n",
    "With linear regression we will try to obtainthe line that best fits our data.\n",
    "\n",
    "To understand what linear regression does we have to understand that the errors of linear regression can be seen as the distance between the prediction line and the real value, this can be both negative or positive.So what we are doing while performing linear regression is to mimimize: $\\sum_{training\\_points} (actual-predicted)^2$ the actual values will be obtained by the training data and the predictions from the regression equation $y=mx+b$ so once we have minimized this equation we will obtain out m and b values corresponding to the slope and the intercept of our regression model.\n",
    "\n",
    "There are several algorithms to obtain this te most popular are:\n",
    "- OLS, Ordinary Least Sqaures (used in sklearn.\n",
    "- Gradient descent.\n",
    "\n",
    "The reason why to obtain the linear regresion we use the **SSE** Sumatory of Squared Errors, is because of two reasons:\n",
    "- It gives only a posible line while without the sqaured it will give us a range.\n",
    "- Is less computationally expensive.\n",
    "\n",
    "Nevertheless SSE has a problem that is that when we introduce more points to the summatory the value of th SSE will probably increment without meaning that its performance is being worse and that's why another performance metric known as the **R-sqaured** is used.\n",
    "\n",
    "$ 0.0 \\ <\\ r^2\\ <\\ 1.0$\n",
    "\n",
    "The $r^2$ answers the question of how much the change in the output is explained by the change in my input. A low $r^2$ means that the line isn't doing a good job capturing the trend in the data, while a high $r^2$ will mean that the line does a good job of describing the relationship between input and output. \n",
    "\n",
    "R_squared is independet to the number of data point on the data and can be negative if the performance is worse than guessing. It can be calculated in sklearn with `reg.score(features, predictions)` once weve fited it into reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  0.5]\n",
      "1.11022302463e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Can also from sklearn.linear_model import LinearRegression and use it directly\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0,0],[1,1],[2,2]],[0,1,2])\n",
    "# Default parameters LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "print(reg.coef_) # This will give the slope\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing regression\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(features_train, predictions_train, color=\"b\", label=\"data\") # We build a scatter plot of the data\n",
    "plt.plot(features_test, reg.predict(features_test), color=\"black\") # We build a line of the predicted values\n",
    "plt.xlabel(\"ages\")\n",
    "plt.ylabel(\"net worths\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate regression\n",
    "\n",
    "In this case our regression will be a linear combination of our different input variables($x$).\n",
    "\n",
    "$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$\n",
    "\n",
    "Across the module, we designate the vector $w = (w_1, ..., w_p)$ as coef\\_ of the different input valriables and $w_0$ as intercept_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Outliers \n",
    "Before talking about outliers detection on linear regression we have to understand what the residual error is. Residual error is the error of a point according to it's regression line. Knowing this we can see the three phases of the outlier rejection process:\n",
    "1. Train\n",
    "2. Remove points with larger residual error 10% of them\n",
    "3. Retrain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
