{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACINE LEARNING INTRO UDACITY\n",
    "\n",
    "---\n",
    "\n",
    "In machine learning we input **features** and we output **labels**. When we are training our model it is important that we train and test our model with different data if not this could couse an overfitting problem. Another problem of machine learning is known as the high dimensionality, that can be solved with the use of feature extraction.\n",
    "\n",
    "### Bias-Variance dilemma\n",
    "\n",
    "#### BIAS:\n",
    "- Pays low attention to data.\n",
    "\n",
    "#### VARIANCE:\n",
    "- Very perceptive to data (reacts very poorly to data that hasn't seen, is more like memoryzing what is ghas seen).\n",
    "\n",
    "### Feature scalling\n",
    "Is used to make different features comparable one with each oder, for example if we make a relation between height and weight height if not scaled will allways be dominant as it tends to have higher values. Below w ecan see how we will compute the scaled value of $x \\rightarrow x^i$\n",
    "\n",
    "$x^i = (x - x_{min}) /(x_{max}-x_{min})$\n",
    "\n",
    "---\n",
    "# Supevised Machine Learning\n",
    "It can be applied to two different tasks **regression** (predict a continuous value) or **classification** (classify data onto a group).\n",
    "- [Naive Bayes](#nb)\n",
    "- [Support Vector Machines](#svm)\n",
    "- [Decision Trees](#dt)\n",
    "- [K-Nearest Neighbours](#knn)\n",
    "- [Random Forest](#rf)\n",
    "- [ADABoost](#ab)\n",
    "\n",
    "- [Linear Regression](#lr)\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"nb\"></a>Naive Bayes \n",
    "\n",
    "Is an algorithm to obtain the decision surface line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB() # We create a classifier\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to calcultae the **accuracy** in a naive bayes classifier is by observing the number of points classified correctly divided by the number of points in the data set. In this case we can't actually do it as we don't see the naive bayes. there is a module in sklearn that can help us to do this \n",
    "``` python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_data, predicted_data)\n",
    "```\n",
    "\n",
    "\n",
    "#### Bayes Rule\n",
    "\n",
    "P( X | Y ) can be seen as the probability of X being Y correct:\n",
    "\n",
    "**Joint: **$P(X,Y)=P(X)*P(Y|X)$\n",
    "\n",
    "**Normalizer: **$P(Y)=P(X|Y)+P(¬X|Y)$\n",
    "\n",
    "$P(X|Y)={P(X,Y)}/{P(Y)} = {P(X)*P(Y|X)}/{P(Y)}$\n",
    "\n",
    "\n",
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.\n",
    "\n",
    "---\n",
    "### <a name=\"svm\"></a>Support Vector Machines\n",
    "\n",
    "Maximizes the distance to the nearest point of the different classes, this distance is called the **margin**. By doing this we maximize the robustness of our results. There are different SVM's **linear** or **non-linear**.\n",
    "![SVM](SVM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\") # There are 4 differnt types of kernels\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf.fit(X, y) \n",
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are treating with non-linear SVM's what we are doing is kind of adding another feature to the SVM, before we had the **X** and **Y** axis that helped us difference different classes linearly. Now we would have also **Z**.  $\\ Z=X^2+Y^2$ so Z will be the distance from the center to the point.\n",
    "\n",
    "**SVM PARAMETERS**\n",
    "- **C**: Low C makes the decision surface smooth, this means it doesn't matter if there are wrong training classifications, high C aims to classify all the training data correctly.\n",
    "- **gamma**: Stablishs the influence of the different points when deciding the decision boundary of the SVM, low values mean that also the far point to the decision boundary are taken into account and a high value means that as closer they are to the decision boundary more influence they have.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <a name=\"dt\"></a>Descision Trees\n",
    "\n",
    "The decision trees allow us to make not linearbly separable data linearly separable. Segmenting the data into different decision. The problem of decision trees is that they tend to overfight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy \n",
    "\n",
    "The entropy helps us to calculate the impurenes of data.\n",
    "\n",
    "$Entropy= -\\sum Pi*\\log_2(Pi)$\n",
    "\n",
    "Pi will be the probability of case i of occuring and the summatory will go through all of the posibilities. For exampleif we are throwing a coin there will be 2 Pi's, heads and tails. It will give an entropy of 1.  \n",
    "\n",
    "#### Information Gain\n",
    "\n",
    "This is usefull to know the information gain of a parent with his childs. The value goes from 0 to 1. 1 means that the information gained between parent and child is maximum what means that there results are totally related.\n",
    "\n",
    "$Information\\ Gain = Entropy\\_Parent - Weighted\\_Average * Entropy\\_Children$ \n",
    "\n",
    "#### Decision tree parameters\n",
    "- **min_samples_split**: stablishs When does the tree stop spliting the data. \n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"knn\"></a>K Nearest Neighbours\n",
    "\n",
    "Basically this algorithm divides the region depending on the elements that it has nearer, K indicates the number of elements that is will look for having it closer, increasing K can help us reduce overfitting. In the image bellow it shows a distribution with **k = 1** at the left and with **k = 3** at the right, k is recommended to be a prime number.\n",
    "![KNN Algoritm](KNNrepresentation.png)\n",
    "\n",
    "There is a concept related with K Nearest Neighbours known as **Kernel Desnsity Estimation** here we select a kernel that stablishs how the distance to the center affects it, the different kernels that are usually seen are the following. So this kernel will be applied from every center to all the points and it will affect more or less depending on the distance.\n",
    "![Kernels](Kernels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 0.66666667  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = [[0],[1],[2],[3]]\n",
    "Y = [0,0,1,1]\n",
    "clf = KNeighborsClassifier(n_neighbors=3) #Stablishs K\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[1.1]]))\n",
    "print(clf.predict_proba([[0.9]])) #Returns the probability of being 0 and of being 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN parameters\n",
    "- **n_neihbors**: Stablishs K.\n",
    "- **weights**: _'uniform'_ all the neighbours have the same weight (default) and _'distance'_ the neighbours that are closer have a higher weight.\n",
    "- **algorithm**: here it is specified the algorithm used to obtain the nearest trees the default is _'auto'_ that decides the most appropiate algorithm based on the values.\n",
    "- **leaf_size**: default = 30, it is passed to BallTree or KDTree that are the algorithms used to compute the nearest tree.\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"rf\"></a>Random Forest\n",
    "\n",
    "The idea of random forest is to add randomness to the decision trees in order to avoid overfitting, reducing the variance of the simple decision trees. Random forest generates different decision trees with the use of random data after that to predict new data it makes a decision based on the result obtained by all the trees and the predominant opinion will be the selected one. [Link](https://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)\n",
    "\n",
    "#### Randomness\n",
    "- **At row level**: A percentage of the data is selected for every tree for example 10% so every random tree generated will use 10% of the data randomly selected. The dataset used for each random forrest is called a bootstrap data set.\n",
    "- **At column level**: It selects X random features of the decision trees, then the root of that decision tree will be one of those X features, the one that better separates the samples of the decision tree. It will do this until it has builed the hole decision trees always selecting between X random features instead of selecting between all of them. Brieman suggests tree posible values for X, being m the number of remaining features: $1/2*\\sqrt m ,\\ \\sqrt m\\ and\\ 2*\\sqrt m  $. Being X allways lower than the number of remaining features (m) except when there is just one feature (m == 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X, Y)\n",
    "print(clf.predict([[0.5,0.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest parameters\n",
    "- **n_estimators**: Number of trees in the forest (default=10).\n",
    "- **criterion**: Is the criteria used to determine which is the best split_'gini'_ (default) or _'entropy'_.\n",
    "- **max_features**: Is the number of features to consider when doing the next split (default=_'auto', max_features = sqrt(num_features)).\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"ab\"></a>AdaBoost\n",
    "\n",
    "Aims to convert a set of weak classifiers into a strong one. It uses weighted classifiers and improves them. What it does is basically it classify the data given and analyzes the error after analyzing them the weight of the correct values is decreased and the weight of th incorrect values is increased so that the algorithm priortizes the ones that weok worst, to try to solve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = AdaBoostClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "print(clf.predict([[1,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost parameters\n",
    "- **base_estimator**: It stablishs the classifier that will be used as weak classifier to be boosted (default=_'DecisionTreeClassifier'_)\n",
    "- **n_estimators**: The maximum number of estimators created, if perfect fit it stops (default = 50).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## <a name=\"lr\"></a>Linear Regression\n",
    "\n",
    "Is a type of Continuous Supervised Learning. The continuous concept, makes reference to the output of the algoithm. Previously the output gave us a discrete output (can be individually separated and distincted and unordered), now we will work with continuous output.\n",
    "\n",
    "With linear regression we will try to obtain the line that best fits our data.\n",
    "\n",
    "To understand what linear regression does we have to understand that the errors of linear regression can be seen as the distance between the prediction line and the real value, this can be both negative or positive.So what we are doing while performing linear regression is to mimimize: $\\sum_{training\\_points} (actual-predicted)^2$ the actual values will be obtained by the training data and the predictions from the regression equation $y=mx+b$ so once we have minimized this equation we will obtain out m and b values corresponding to the slope and the intercept of our regression model.\n",
    "\n",
    "There are several algorithms to obtain this te most popular are:\n",
    "- OLS, Ordinary Least Sqaures (used in sklearn.\n",
    "- Gradient descent.\n",
    "\n",
    "The reason why to obtain the linear regresion we use the **SSE** Sumatory of Squared Errors, is because of two reasons:\n",
    "- It gives only a posible line while without the sqaured it will give us a range.\n",
    "- Is less computationally expensive.\n",
    "\n",
    "Nevertheless SSE has a problem that is that when we introduce more points to the summatory the value of th SSE will probably increment without meaning that its performance is being worse and that's why another performance metric known as the **R-sqaured** is used.\n",
    "\n",
    "$ 0.0 \\ <\\ r^2\\ <\\ 1.0$\n",
    "\n",
    "The $r^2$ answers the question of how much the change in the output is explained by the change in my input. A low $r^2$ means that the line isn't doing a good job capturing the trend in the data, while a high $r^2$ will mean that the line does a good job of describing the relationship between input and output. \n",
    "\n",
    "R_squared is independet to the number of data point on the data and can be negative if the performance is worse than guessing. It can be calculated in sklearn with `reg.score(features, predictions)` once weve fited it into reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  0.5]\n",
      "1.11022302463e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Can also from sklearn.linear_model import LinearRegression and use it directly\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0,0],[1,1],[2,2]],[0,1,2])\n",
    "# Default parameters LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "print(reg.coef_) # This will give the slope\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing regression\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(features_train, predictions_train, color=\"b\", label=\"data\") # We build a scatter plot of the data\n",
    "plt.plot(features_test, reg.predict(features_test), color=\"black\") # We build a line of the predicted values\n",
    "plt.xlabel(\"ages\")\n",
    "plt.ylabel(\"net worths\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate regression\n",
    "\n",
    "In this case our regression will be a linear combination of our different input variables($x$).\n",
    "\n",
    "$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p = \\sum_{j=0}^{p}w_j*x^j$\n",
    "\n",
    "Across the module, we designate the vector $w = (w_1, ..., w_p)$ as coef\\_ of the different input valriables and $w_0$ as intercept_.\n",
    "\n",
    "### Outliers \n",
    "\n",
    "Before talking about outliers detection on linear regression we have to understand what the residual error is. Residual error is the error of a point according to it's regression line. Knowing this we can see the three phases of the outlier rejection process:\n",
    "1. Train.\n",
    "2. Remove points with larger residual error 10% of all data points.\n",
    "3. Retrain.\n",
    "\n",
    "### Error calculation\n",
    "\n",
    "When calculating the error of a regression we have two options:\n",
    "1. **MSE**, Mean Squared Error, this will be equal to $r^2$\n",
    "\n",
    "$E_{MSE}= 1 / N * \\sum_{i=1}^N(y_i-predict_i)^2$\n",
    "```python\n",
    "return  numpy.dot(temp_,temp_) / len(temp_)\n",
    "```\n",
    "\n",
    "2. **RMS**, Root Mean Square, is the most recommended calculus as is the most interpretable.\n",
    "\n",
    "$E_{RMS}= \\sqrt {1/N * \\sum_{i=1}^N(y_i-predict_i)^2}$\n",
    "```python\n",
    "return numpy.sqrt(numpy.dot(temp_,temp_) / len(temp_) )\n",
    "```\n",
    "\n",
    "---\n",
    "# Unsupervised  Machine Learning\n",
    "\n",
    "- [Clustering](#c)\n",
    "- Dimensionality reduction\n",
    "- Density estimation.\n",
    "---\n",
    "\n",
    "## <a name=\"c\"></a>Clustering\n",
    "\n",
    "When we are executing clustering, we basicaly group the data into different clusters. We can use different algorithms that execute this technique.\n",
    "- [K-Means](#km)\n",
    "\n",
    "### <a name=\"km\"></a>K-Means\n",
    "\n",
    "The most famous algorithm to execute clustering is **K-means** this algorithm is executed on two steps that are repeated constantly until the clusters are made. First we generate x random center poitns ( x is equal to the number of clusters on the data).\n",
    "1. We **assign** the data point to it's closer ccenter point.\n",
    "2. We **optimize** (reduce) the distance from the center points to there assigned data points.\n",
    "![k-means](k-means.png)\n",
    "[Check out this link to play with kmeans clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n",
    "\n",
    "As the reassign points and update centroids steps are linear, the practical run time of the algorithm is basically linear. \n",
    "Unfortuantly the final cluster configuration isn't deterministic and depends on the initial position of the center points generated. Thats the reason why people will try several initialization strategies, (or try a randomized initialization strategy multiple times), and pick the one that results in the best clustering. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1]\n",
      "[0 1]\n",
      "[[ 1.  2.]\n",
      " [ 4.  2.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.predict([[0, 0], [4, 4]]))\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### K-Means parameters\n",
    "- **n_cluster**: Number of clusters to form, will determine the number of centers genrated (default = 8).\n",
    "- **init**: The mmethod of initialization _'k-means++'_ (default), _'random'_ or passing an ndarray that gives the initial centers.\n",
    "- **n_init**: is the number of time the algorithm is initialized, the output will be the one who performed best in terms of inertia (sum of squared distances of samples to their closest center) (default = 10).\n",
    "- **max_iter**: maximum numbers of iterations of the algorithm (default = 300).\n",
    "\n",
    "#### Counterpoints\n",
    "- It doesn't allow us to diferentiate between distributions that are one over the other, this can be achieved with Gaussian Mixture Models **GMM**.\n",
    "\n",
    "---\n",
    "## Text Learning\n",
    "\n",
    "#### Bag of words\n",
    "It makes a bag of all the words appearing on the text with there frequency of appearance\n",
    "#### Stemmer\n",
    "It basically help us obtain the root of the words as we don't have different words that have just been slightly changed, for example live and lives will be changed to live.\n",
    "#### TfIdf representation\n",
    "It rates the rare words with higher weights than the other words.\n",
    "\n",
    "---\n",
    "## Feature selection\n",
    "You want to capture the least features as posible that allow you obtain the maximum accurcacy possible.\n",
    "\n",
    "**Reasons why we might want to get rid of a feature:**\n",
    "- It's nosiy\n",
    "- It causes overfitting.\n",
    "- It is strongly related (highly correlated) with a feature that's all ready present.\n",
    "- Additional features slow down training/testing process.\n",
    "\n",
    "There are several univariate feature selection techniques, which treats each feature independently and asks how much power it gives you in classifying or regressing. There are two big univariate feature selection tools in sklearn: \n",
    "- **SelectPercentile**: selects the X% of features that are most powerful (where X is a parameter).\n",
    "- **SelectKBest**: selects the K features that are most powerful (where K is a parameter). . \n",
    "\n",
    "#### Measurable vs Latent features\n",
    "- **Measurable**: can be directly measured.\n",
    "- **Latent**: they can be obtained thanks to the measurements of other features and they contain most of the information needed, for example the quality of a neighbourhood.\n",
    "\n",
    "The features can also be seen as the dimensions of the model. We can increase or decrease de dimensions/features of the model, obtaining to different trends:\n",
    "\n",
    "- $\\downarrow$ dimensions $\\rightarrow$ Under-fit, high-bias.\n",
    "- $\\uparrow$ dimensions $\\rightarrow$ Over-fit, high variance.\n",
    "![dimension](dimensionality.png)\n",
    "So the objective is to find the point were the performance of the model is maximum, obtaining the dimensionality that best fits the model, this is called **regularization**.\n",
    "\n",
    "### Regularization\n",
    "#### Lasso Regression\n",
    "Lasso Regression is a way in which we can execute regularization while working on our regression model. The objective in lasso regression is to minimize the following equation:\n",
    "\n",
    "$\\text{SSE} + \\lambda \\  |\\beta|$ \n",
    "\n",
    "So this formula minimizes the error of the actual regression $\\text{SSE}$ and the error of adding more featuers $\\lambda \\ |\\beta|$, $\\lambda \\rightarrow$ paramter and $\\beta \\rightarrow$ coefficient of regression. \n",
    "\n",
    "## PCA\n",
    "\n",
    "PCA is a very powerfull unsupervised learning technique that allow us to execute a dimensionality reduction of our data.\n",
    "\n",
    "**Maximal Variance**: Is the direction that has the largest variance(the spread of the data) on the data set. This lines that gives the maximal variance are known as the **Principal Components**.\n",
    "\n",
    "This maximal variance line will give us the line were we will project the data points so that we execute a dimensionality reduction through PCA. Is done this way because it retains the maximum amount of information of the data.\n",
    "\n",
    "![Maximal Variance](maximal_variance.png)\n",
    "\n",
    "The amount of information lost is the distance between the data points and the maxiaml variance line. So actually what we do by using the maximal variance line is reduce the amount of information lost.\n",
    "\n",
    "- So the PC's are the directions of the data that maximize the variance.\n",
    "- The PC that maximizes more the data will be the one with higher rank.\n",
    "- The PC's don't overlap at all between themselves so they all form 90º between themselves.\n",
    "- The max Nº of PC's is the Nº of input features.\n",
    "\n",
    "When we fit the PCA we just use the training set of data. But we will transform both of them as we needto have both sets on the same dimensionality.\n",
    "\n",
    "## Cross validation\n",
    "### K-fold C.V.\n",
    "When we execute k-fold cross validation we separe the hole data set in k parts and we select one of the parts to be the test data set. So after testing all the parts as test data sets we calculate the average accuracy of all the models. This will allow us to obtain a much more precise result as we take into account all the data point when executing it.\n",
    "![Cross Validation](cross_validation.png)\n",
    "### Gridsearch C.V.\n",
    "GridSearchCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
    "\n",
    "Here's an example from the sklearn documentation:\n",
    "```PYTHON\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "\n",
    "## Evaluation metrics\n",
    "Not all the problems can be evaluated the same way, it is important to understand the problem to be able to stablish good and reliable evaluation metrics. For example it will not be the same to identify a person with cancer as not having cancer that identifying a person without having cancer as having cancer.\n",
    "\n",
    "\n",
    "**RECALL**: True Positive / (True Positive + False Negative). Out of all the items that are truly positive, how many were correctly classified as positive. Or simply, how many positive items were 'recalled' from the dataset.\n",
    "\n",
    "**PRECISION**: True Positive / (True Positive + False Positive). Out of all the items labeled as positive, how many truly belong to the positive class.\n",
    "\n",
    "#### F1-score\n",
    "The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "$F1 = 2 * (precision*recall)/(precision+recall)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
