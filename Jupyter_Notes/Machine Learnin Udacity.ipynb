{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACINE LEARNING INTRO UDACITY\n",
    "\n",
    "---\n",
    "\n",
    "In machine learning we input **features** and we output **labels**. When we are training our classifier it is important that we train and test our classifier with different data if not this could couse an overfitting problem.\n",
    "\n",
    "### Naive Bayes \n",
    "\n",
    "Is an algorithm to obtain the decision surface line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB() # We create a classifier\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to calcultae the **accuracy** in a naive bayes classifier is by observing the number of points classified correctly divided by the number of points in the data set. In this case we can't actually do it as we don't see the naive bayes. there is a module in sklearn that can help us to do this \n",
    "``` python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_data, predicted_data)\n",
    "```\n",
    "\n",
    "### Bayes Rule\n",
    "\n",
    "P( X | Y ) can be seen as the probability of X being Y correct:\n",
    "\n",
    "**Joint: **$P(X,Y)=P(X)*P(Y|X)$\n",
    "\n",
    "**Normalizer: **$P(Y)=P(X|Y)+P(¬X|Y)$\n",
    "\n",
    "$P(X|Y)={P(X,Y)}/{P(Y)} = {P(X)*P(Y|X)}/{P(Y)}$\n",
    "\n",
    "\n",
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
